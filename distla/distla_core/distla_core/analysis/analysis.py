# Copyright 2021 The Distla Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
import csv
import numpy as np
import os
import gc
import functools
import time
from typing import Callable, Sequence


def _parametrized_analysis(init_f: Callable, target_f: Callable,
                           decorator_f: Callable,
                           param_lists: Sequence, param_headers: Sequence,
                           output_dir_path=None,
                           output_name="eval"):

  """
  `parametrized_analysis` runs a decorated target function upon inputs
  generated by a set of parameters and saves the results to disk. The
  results are saved as comma separated values to the file
  `output_dir_path/output_name`.

  In more detail, `parametrized_analysis` effectively performs the following
  operations:
    1. Each param_list is fed to `init_f`, which produces a corresponding
       set of inputs: `in_args = init_f(*param_set)`.
    2. The decorated target is applied to each `in_args`,
       `out = decorator_f(target_f, *in_args)`.
    3. The output is appended to a `.csv` file at the path
       `output_dir_path/output_name`.

  Args:
    `init_f`: A function `in_args = init_f(*param_set, **init_kwargs)` mapping
              each `param_set` to a corresponding set of inputs to
              `target_f` via `decorator_f`.
              Example: a function generating random matrices to be multiplied.
    `target_f`: A function `target_out = target_f(*in_args, **target_kwargs)`
                mapping each generated `in_args` to a set of throughput values.
                Example: a function multiplying matrices.
    `decorator_f`: A function
                  `out = decorator_f(target_f, *param_set, **decorator_kwargs)`
                  that performs a pre- and post-processed function call upon
                  `target_f`. `*in_args` must be passed to `target_f` via
                  this function. `decorator_f` should emit output as two
                  sequences, one of values and one of headers, e.g.
                  `out = ([0.001, 1.4], ["Seconds", "tflops"])`.
                  Example 1: a function to benchmark `target_f` and emit various
                             speed estimates.
                  Example 2: a function to compute various error metrics from
                             `target_f`.
    `param_lists`: Lists of lists, each storing a set of input params.
    `param_headers`: Lists of strings, each representing a param_list name.
    `output_dir_path`, `output_name`:
                Results will be saved as a `.csv` file to
                `output_dir_path/output_name.csv`. `output_dir_path` is the
                current working directory if unspecified.
  Raises:
    `ValueError`: If `decorator_f` did not generate the same headers at each
                  iteration.
                  If `param_lists` and `param_headers` were not the same length.
  Returns:
    None
  """
  if output_dir_path is None:
    output_dir_path = os.getcwd()
  if not os.path.exists(output_dir_path):
    os.makedirs(output_dir_path)

  full_output_path = os.path.join(output_dir_path, output_name + ".csv")
  print("Output will be saved to " + full_output_path)
  print("Generating data.")
  with open(full_output_path, 'w') as outfile:
    writer = csv.writer(outfile)
    for i, params in enumerate(param_lists):
      if len(params) != len(param_headers):
        raise ValueError(f"{i}th param list had length {len(params)}, which did"
                         f" not equal len(param_headers) {len(param_headers)}.")
      out_vals, headers = decorator_f(init_f, target_f, *params)
      pzipped = [header + " = " + str(param) for header, param in
                 zip(param_headers, params)]
      print(*pzipped)
      if i == 0:
        out_headers = headers
        all_headers = param_headers + out_headers
        writer.writerow(all_headers)
      else:
        if headers != out_headers:
          raise ValueError(f"decorator_f emitted headers {headers},"
                           f" which differed from the initial {out_headers}.")
      ozipped = [header + " = " + str(val) for header, val in
                 zip(out_headers, out_vals)]
      print(*ozipped)
      all_vals = list(params) + list(out_vals)
      writer.writerow(all_vals)
  print("Data generation complete.")
  print("Evaluation complete.")


# decorator_fs
def _get_reduction_function(reduction_mode):
  if reduction_mode == "median":
    f = np.median
  elif reduction_mode == "min":
    f = np.amin
  elif reduction_mode == "mean":
    f = np.mean
  else:
    raise ValueError(f"Invalid reduction_mode: {reduction_mode}.")
  return f


def _analysis_decorator(init_f, target_f, *params, batch_size=1,
                        reduction_mode="median", output_functions=None):
  """
  `_analysis_decorator` is a template `decorator_f` argument to
  `parametrized_analysis` that can be used as is or further specialized as
  required.

  `batch_size` calls `out = target_f(*in_args)` will be made to produce output
  for analysis.  Each output will be run through each function in
  `output_fuctions` as `err = out_function(out, in_args, params)` to produce
  estimates of error.

  Each batch of error estimates (one per error function) will be
  reduced into a single estimate according to the choice of `reduction_mode`
  (`"median"` (default), `"min"`, or `"mean"`).
  The result is returned as two lists, one storing the errors and the next
  storing their names.

  Args:
    `target_f`: A function `target_f(*in_args)` to be analyzed.
    `batch_size`: Number of runs used to form each output.
    `reduction_mode`: A string `"median"` (default), `"min"`, or `"mean"`,
                      specifying the measure of central tendency used to
                      assemblage each output over each batch.
    `output_functions`: An optional list of `Callables`
                     `output_function(out, in_args, params)` to produce
                      postprocessed results.

  Raises:
    `ValueError`: If  `output_functions` was not specified.
                  If any `output_functions` returned different headers between
                  the first and subsequent calls.
  Returns:
    outputs: The values of the outputs.
    headers: The names of the outputs.
  """
  in_args = init_f(*params)

  if output_functions is None:
    raise ValueError("output_fuctions must be specified.")
  n_functions = len(output_functions)

  headers = []
  outputs = np.zeros((n_functions, batch_size))
  for i in range(batch_size):
    out = target_f(*in_args)
    for j in range(n_functions):
      this_output, this_header = output_functions[j](out, in_args, params)
      try:
        outputs[j, i] = this_output
      except ValueError as e:
        print("*************************************************************")
        print("_analysis_decorator caught ValueError: ", e)
        print(f"This likely indicates that output_function {j} did not emit ")
        print("scalar output.")
        print(f"The output header was {this_header}.")
        print(f"The output was {this_output}.")
        print("Ramaxcut the error now: ")
        raise
      if i == 0:
        headers.append(this_header)
      else:
        if headers[j] != this_header:
          raise ValueError(f"{j}'th function returned header "
                           f"{this_header} on the {i}th pass, which differed "
                           f"from its initial output {headers[j]}.")
    del out
    gc.collect()

  reduction_f = _get_reduction_function(reduction_mode)
  outputs = list(reduction_f(outputs, axis=1))
  return outputs, headers


def _benchmark_decorator(init_f, target_f, *params, batch_size=1,
                         reduction_mode="median", speed_functions=None):
  """
  `_benchmark_decorator` is a `decorator_f` argument to `parametrized_analysis`
  that runs benchmarks of `target_f`.

  `target_f(*in_args)` will be called once as a warmup. `batch_size` subsequent
  calls will then be made to produce a set of timings in seconds. These will be
  reduced into a single  estimate according to the choice of `reduction_mode`
  (`"median"` (default), `"min"`, or `"mean"`). Finally and optionally, each of
  the given `speed_functions` will be called upon `*in_args` and the estimated
  time (`speed_func(*in_args, estimated_time)`) to produce estimates of
  speed.

  Args:
    `init_f`: A function to generate arguments to `target_f` from `*params`.
    `target_f`: A function to be benchmarked.
    `*params`: Positional arguments to `init_f`.
    `batch_size`: Number of runs used to form the time estimate.
    `reduction_mode`: A string `"median"` (default), `"min"`, or `"mean"`,
                      specifying the measure of central tendency used to
                      form the time estimate.
    `speed_functions`: An optional list of `Callables`
                       `speed_func(estimated time, out, in_args, params)` to
                       to produce postprocessed results from the benchmarked
                       times. These functions should return the speed estimate
                       and a header, e.g. 0.1, "Tflop/s".


  Returns:
    out: The various numerical estimates of speed, starting with the runtime
         in seconds and continuing with the output of `speed_functions` in
         order.
    headers: The names of these estimates, starting with `Seconds` and
             continuing with the headers output by `speed_functions` in order.
  """
  def _timed_target_f(*in_args):
    start = time.time()
    out = target_f(*in_args)
    dt = time.time() - start
    return dt, *out

  def _dt_function(out, in_args, params):
    dt = out[0]
    return dt, "Seconds"

  if speed_functions is None:
    output_functions = [_dt_function, ]
  else:
    output_functions = [_dt_function, ] + speed_functions

  return _analysis_decorator(init_f, _timed_target_f, *params,
                             batch_size=batch_size,
                             reduction_mode=reduction_mode,
                             output_functions=output_functions)


###############################################################################
# INTERFACE
###############################################################################
def benchmark(init_f, target_f, param_lists, param_headers,
              batch_size=5, reduction_mode="median",
              speed_functions=None,
              output_dir_path=None,
              output_name="benchmark_output"):
  """
  `benchmark` takes benchmarks of a target function `target_f` initialized
  by `init_f`, itself parametrized by `param_lists`. For each `param_list`
  in `param_lists`, `target_f(init_f(param_list))` will be run `batch_size`
  times, yielding runtimes in seconds to be combined according to
  `reduction_mode`. Estimates of speed are then optionally generated by
  a list of `speed_functions`, each of the form
  `speed, header = speed_function(time, param_list)`. The params, param_headers,
  times, speeds, and speed_headers are then combined into a DataFrame, which
  is both returned and saved to `output_dir_path/output_name.csv`. If
  not supplied, `output_dir_path` is the current working directory.

  More explicit descriptions of behaviour can be found in the docstrings to
  `_benchmark_decorator` and `_parametrized_analysis`, which this function
  combines.

  Args:
    `init_f`: A function `in_args = init_f(*param_set, **init_kwargs)` mapping
              each `param_set` to a corresponding set of inputs to
              `target_f` via `decorator_f`.
              Example: a function generating random matrices to be multiplied.
    `target_f`: A function `target_out = target_f(*in_args, **target_kwargs)`
                mapping each generated `in_args` to a set of throughput values.
                Example: a function multiplying matrices.
    `param_lists`: Lists of lists, each storing a set of input params.
    `param_headers`: Lists of strings, each representing a param_list name.
    `reduction_mode`: A string `"median"` (default), `"min"`, or `"mean"`,
                      specifying the measure of central tendency used to
                      form the time estimate.
    `speed_functions`: An optional list of `Callables`
                       `speed_func(estimated time, out, in_args, params)` to
                       produce postprocessed results from the benchmarked
                       times. These functions should return the speed estimate
                       and a header, e.g. 0.1, "Tflop/s".
    `output_dir_path`, `output_name`:
                Results will be saved as a `.csv` file to
                `output_dir_path/output_name.csv`. output_dir_path is the
                current working directory if unspecified.
  Raises:
    `ValueError`: If `param_lists` and `param_headers` were not the same length.

  Returns:
    `out`: The `DataFrame` used to generate the `.csv` file.
  """
  decorator_f = functools.partial(_benchmark_decorator,
                                  reduction_mode=reduction_mode,
                                  batch_size=batch_size,
                                  speed_functions=speed_functions)
  return _parametrized_analysis(init_f, target_f, decorator_f,
                                param_lists, param_headers,
                                output_dir_path=output_dir_path,
                                output_name=output_name)


def measure_error(init_f, target_f, param_lists, param_headers,
                  batch_size=5, reduction_mode="median",
                  error_functions=None,
                  output_dir_path=None,
                  output_name="measure_error.csv"):
  """
  `measure_error` computes error estimates of a target function `target_f`
  initialized by `init_f`, itself parametrized by `param_lists`. For each
  `param_list` in `param_lists`, `in = init_f(param_list)` and then
  `out = target_f(in)` will run `batch_size` times, yielding runtimes in seconds
  to be combined according to `reduction_mode`. Error estimates are then
  generated by a list of `error_functions`, each of the form
  `error, header = error_function(out, in, param_list)`. The params,
  param_headers, errors, and error_headers are then combined into a DataFrame,
  which is both returned and saved to `output_dir_path/output_name.csv`. If
  not supplied, `output_dir_path` is the current working directory.

  More explicit descriptions of behaviour can be found in the docstrings to
  `_error_decorator` and `_parametrized_analysis`, which this function
  combines.

  Args:
    `init_f`: A function `in_args = init_f(*param_set, **init_kwargs)` mapping
              each `param_set` to a corresponding set of inputs to
              `target_f` via `decorator_f`.
              Example: a function generating random matrices to be multiplied.
    `target_f`: A function `target_out = target_f(*in_args, **target_kwargs)`
                mapping each generated `in_args` to a set of throughput values.
                Example: a function multiplying matrices.
    `param_lists`: Lists of lists, each storing a set of input params.
    `param_headers`: Lists of strings, each representing a param_list name.
    `reduction_mode`: A string `"median"` (default), `"min"`, or `"mean"`,
                      specifying the measure of central tendency used to
                      form the error estimate.
    `error_functions`: An optional list of `Callables`
                       `error_func(out, in_args, params)` to produce
                       postprocessed results.

    `output_dir_path`, `output_name`:
                Results will be saved as a `.csv` file to
                `output_dir_path/output_name.csv`. output_dir_path is the
                current working directory if unspecified.
  Raises:
    `ValueError`: If `param_lists` and `param_headers` were not the same length.

  Returns:
    `out`: The `DataFrame` used to generate the `.csv` file.
  """
  decorator_f = functools.partial(_analysis_decorator,
                                  reduction_mode=reduction_mode,
                                  batch_size=batch_size,
                                  output_functions=error_functions)
  return _parametrized_analysis(init_f, target_f, decorator_f,
                                param_lists, param_headers,
                                output_dir_path=output_dir_path,
                                output_name=output_name)
